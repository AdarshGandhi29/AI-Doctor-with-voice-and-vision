# üè• AI Doctor 2.0 ‚Äì Vision & Voice Based Multimodal Chatbot

A multimodal AI-powered medical chatbot that enables patients to interact using **text**, **voice**, and **medical images**. The system integrates **LLMs**, **speech models**, and **vision models** to simulate intelligent, real-time doctor consultation experiences.

---

## üöÄ Features

- üß† **Multimodal Inference**: Combines text, voice, and image inputs using **LLaMA 3 Vision** via **GROQ API**.
- üéôÔ∏è **Voice Input/Output**: Enables full duplex interaction using **Whisper (STT)** and **gTTS / ElevenLabs (TTS)**.
- üñºÔ∏è **Medical Image Understanding**: Accepts diagnostic images (e.g., X-rays) for intelligent analysis.
- üí¨ **Interactive UI**: Built using **Gradio** for real-time communication and seamless UX.
- üîÑ **End-to-End Workflow**: Voice/Image ‚Üí Preprocessing ‚Üí LLM ‚Üí Voice/Text Response.

---

## üß† System Architecture

- [Voice Input] ‚Üí Whisper STT
- [Image Input] ‚Üí Preprocessing
- [Text + Image Input] ‚Üí LLaMA 3 Vision via GROQ API
- [LLM Output] ‚Üí gTTS / ElevenLabs TTS
- [UI] ‚Üí Gradio (real-time interaction)

---

## üõ†Ô∏è Tech Stack

- Python
- Gradio (UI)
- GROQ API (LLM inference)
- LLaMA 3 Vision (multimodal model)
- Whisper (Speech-to-Text)
- gTTS / ElevenLabs (Text-to-Speech)
- ffmpeg & portaudio (audio recording)
- PIL / ImageLib (image preprocessing)

---

## üì¶ Project Structure

ai-doctor-2.0/
‚îú‚îÄ‚îÄ app.py # Main logic for Gradio interface
‚îú‚îÄ‚îÄ llm_inference.py # GROQ + LLaMA 3 Vision call logic
‚îú‚îÄ‚îÄ speech_utils.py # Whisper STT and TTS integration
‚îú‚îÄ‚îÄ image_utils.py # Image preprocessing
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

---

## üìà Future Improvements

- Finetune vision model on medical datasets
- Add multilingual voice/text support
- Upgrade to premium LLMs for enhanced accuracy

---

> ‚ö†Ô∏è Disclaimer: This project is a prototype and not intended for real-world clinical use.
